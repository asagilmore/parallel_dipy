{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import os.path as op\n",
    "import dipy.reconst.csdeconv as csd\n",
    "from dipy.data.fetcher import fetch_hcp\n",
    "from dipy.core.gradients import gradient_table\n",
    "import nibabel  as nib\n",
    "import time\n",
    "from dipy.reconst.csdeconv import (auto_response_ssst,\n",
    "                                   mask_for_response_ssst,\n",
    "                                   response_from_mask_ssst)\n",
    "from dipy.align import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asagilmore/.dipy/HCP_1200/derivatives/hcp_pipeline/sub-100307\n"
     ]
    }
   ],
   "source": [
    "## fetch subject data\n",
    "subject = 100307\n",
    "dataset_path = fetch_hcp(subject)[1]\n",
    "subject_dir = op.join(\n",
    "    dataset_path,\n",
    "    \"derivatives\",\n",
    "    \"hcp_pipeline\",\n",
    "    f\"sub-{subject}\",\n",
    "    )\n",
    "print(subject_dir)\n",
    "subject_files = [op.join(subject_dir, \"dwi\",\n",
    "        f\"sub-{subject}_dwi.{ext}\") for ext in [\"nii.gz\", \"bval\", \"bvec\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "dwi_img = nib.load(subject_files[0])\n",
    "data = dwi_img.get_fdata()\n",
    "seg_img = nib.load(op.join(\n",
    "    subject_dir, \"anat\", f'sub-{subject}_aparc+aseg_seg.nii.gz'))\n",
    "seg_data = seg_img.get_fdata()\n",
    "brain_mask = seg_data > 0\n",
    "dwi_volume = nib.Nifti1Image(data[..., 0], dwi_img.affine)\n",
    "brain_mask_xform = resample(brain_mask, dwi_volume,\n",
    "                            moving_affine=seg_img.affine)\n",
    "brain_mask_data = brain_mask_xform.get_fdata().astype(int)\n",
    "gtab = gradient_table(subject_files[1], subject_files[2])\n",
    "response_mask = mask_for_response_ssst(gtab, data, roi_radii=10, fa_thr=0.7)\n",
    "response, _ = response_from_mask_ssst(gtab, data, response_mask)\n",
    "\n",
    "\n",
    "csdm = csd.ConstrainedSphericalDeconvModel(gtab, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536]\n"
     ]
    }
   ],
   "source": [
    "data = pd.Dataframe(columns=[\"engine\",\"vox_per_chunk\",\"time\"])\n",
    "\n",
    "\n",
    "## run csdm with the given engine and vox_per_chunk\n",
    "# appends the given time, engine, and vox_per_chunk to the data dataframe\n",
    "# returns the time it took to run\n",
    "def run_csdm(engine, vox_per_chunk):\n",
    "    global data\n",
    "\n",
    "    start = time.time()\n",
    "    csdm.fit(data, mask=brain_mask_data, engine=engine, vox_per_chunk=vox_per_chunk)\n",
    "    end = time.time()\n",
    "\n",
    "    time = end-start\n",
    "\n",
    "    data.append({\"engine\":engine,\"vox_per_chunk\":vox_per_chunk,\"time\":time})\n",
    "\n",
    "    return time\n",
    "\n",
    "\n",
    "\n",
    "engines = [\"ray\",\"joblib\",\"dask\",\"serial\"]\n",
    "\n",
    "vox_per_chunk = [2**i for i in range(0,17)]\n",
    "\n",
    "print(vox_per_chunk)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Runs the csdm model with the given engine and vox_per_chunk until a certain confidence interval is reached\n",
    "for a certain confidence level.\n",
    "Here we assume that the distribution of error is roughly gassian, this has yet to be tested.\n",
    "\n",
    "Args:\n",
    "engine (str): the engine to use for parallelization\n",
    "vox_per_chunk (int): the number of voxels to process in each chunk\n",
    "conf_int (float): the confidence interval to reach as a percentage of mean\n",
    "conf_level (float): the confidence level to reach as a percentage\n",
    "\"\"\"\n",
    "def compute_to_confidence(engine, vox_per_chunk, conf_int, conf_level=0.95,max_iter=30):\n",
    "    times = []\n",
    "\n",
    "    # z score for conf level\n",
    "    Z = norm.ppf((1 + conf_level) / 2)\n",
    "\n",
    "    ##run a couple times to get standard deviation\n",
    "    for i in range(0,6):\n",
    "        times.append(run_csdm(engine, vox_per_chunk))\n",
    "\n",
    "    times = pd.Series(times)\n",
    "    mean = times.mean()\n",
    "    std = times.std()\n",
    "\n",
    "    margin_err = conf_int * mean\n",
    "\n",
    "    n = math.ceil((Z * std / conf_int) ** 2)\n",
    "\n",
    "\n",
    "    # if we have enough samples, return,\n",
    "    # else run more until we do\n",
    "    # if we run more than max_iter times, return\n",
    "    if (times.length >= n):\n",
    "        return\n",
    "    else:\n",
    "        while(times.length < n):\n",
    "            times.append(run_csdm(engine, vox_per_chunk))\n",
    "            mean = times.mean()\n",
    "            std = times.std()\n",
    "            margin_err = conf_int * mean\n",
    "            n = math.ceil((Z * std / conf_int) ** 2)\n",
    "            if(n > max_iter):\n",
    "                return\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipy",
   "language": "python",
   "name": "dipy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
