{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports\n",
    "import os.path as op\n",
    "import dipy.reconst.csdeconv as csd\n",
    "from dipy.data.fetcher import fetch_hcp\n",
    "from dipy.core.gradients import gradient_table\n",
    "import nibabel  as nib\n",
    "import time\n",
    "from dipy.reconst.csdeconv import (auto_response_ssst,\n",
    "                                   mask_for_response_ssst,\n",
    "                                   response_from_mask_ssst)\n",
    "from dipy.align import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/asagilmore/.dipy/HCP_1200/derivatives/hcp_pipeline/sub-100307\n"
     ]
    }
   ],
   "source": [
    "## fetch subject data\n",
    "subject = 100307\n",
    "dataset_path = fetch_hcp(subject)[1]\n",
    "subject_dir = op.join(\n",
    "    dataset_path,\n",
    "    \"derivatives\",\n",
    "    \"hcp_pipeline\",\n",
    "    f\"sub-{subject}\",\n",
    "    )\n",
    "print(subject_dir)\n",
    "subject_files = [op.join(subject_dir, \"dwi\",\n",
    "        f\"sub-{subject}_dwi.{ext}\") for ext in [\"nii.gz\", \"bval\", \"bvec\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load data\n",
    "dwi_img = nib.load(subject_files[0])\n",
    "data = dwi_img.get_fdata()\n",
    "seg_img = nib.load(op.join(\n",
    "    subject_dir, \"anat\", f'sub-{subject}_aparc+aseg_seg.nii.gz'))\n",
    "seg_data = seg_img.get_fdata()\n",
    "brain_mask = seg_data > 0\n",
    "dwi_volume = nib.Nifti1Image(data[..., 0], dwi_img.affine)\n",
    "brain_mask_xform = resample(brain_mask, dwi_volume,\n",
    "                            moving_affine=seg_img.affine)\n",
    "brain_mask_data = brain_mask_xform.get_fdata().astype(int)\n",
    "gtab = gradient_table(subject_files[1], subject_files[2])\n",
    "response_mask = mask_for_response_ssst(gtab, data, roi_radii=10, fa_thr=0.7)\n",
    "response, _ = response_from_mask_ssst(gtab, data, response_mask)\n",
    "\n",
    "\n",
    "csdm = csd.ConstrainedSphericalDeconvModel(gtab, response=response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import math\n",
    "from scipy.stats import norm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "\n",
    "runTimeData = []\n",
    "\n",
    "## run csdm with the given engine and vox_per_chunk\n",
    "# appends the given time, engine, and vox_per_chunk to the data dataframe\n",
    "# returns the time it took to run\n",
    "def run_csdm(engine, vox_per_chunk,save = True):\n",
    "\n",
    "    global runTimeData\n",
    "\n",
    "    start = time.time()\n",
    "    csdm.fit(data, mask=brain_mask_data, engine=engine, vox_per_chunk=vox_per_chunk)\n",
    "    end = time.time()\n",
    "\n",
    "    runTime = end-start\n",
    "\n",
    "    if (save):\n",
    "        runTimeData.append({'engine':engine,'vox_per_chunk':vox_per_chunk,'time':runTime})\n",
    "    else:\n",
    "        print(\"save turned off, runTime not saved\")\n",
    "\n",
    "    print(\"engine: \", engine, \" vox_per_chunk: \", vox_per_chunk, \" time: \", runTime)\n",
    "\n",
    "    return runTime\n",
    "\n",
    "def save_data(filename):\n",
    "    global runTimeData\n",
    "\n",
    "    # specify the names for CSV column headers\n",
    "    fieldnames = runTimeData[0].keys() if runTimeData else Error(\"No data to save\")\n",
    "\n",
    "    # writing to csv file\n",
    "    with open(filename, 'a', newline='') as csvfile:\n",
    "        # creating a csv writer object\n",
    "        csvwriter = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "\n",
    "        # writing headers (field names) if the file doesn't exist or it is empty\n",
    "        if not os.path.isfile(filename) or os.path.getsize(filename) == 0:\n",
    "            csvwriter.writeheader()\n",
    "\n",
    "        # writing the data rows\n",
    "        csvwriter.writerows(runTimeData)\n",
    "\n",
    "    runTimeData.clear()\n",
    "\n",
    "\"\"\"\n",
    "Runs the csdm model with the given engine and vox_per_chunk until a certain confidence interval is reached\n",
    "for a certain confidence level.\n",
    "Here we assume that the distribution of error is roughly gassian, this has yet to be tested.\n",
    "\n",
    "Args:\n",
    "engine (str): the engine to use for parallelization\n",
    "vox_per_chunk (int): the number of voxels to process in each chunk\n",
    "conf_int (float): the confidence interval to reach as a percentage of mean\n",
    "conf_level (float): the confidence level to reach as a percentage\n",
    "\"\"\"\n",
    "def compute_to_confidence(engine, vox_per_chunk, conf_int, conf_level=0.95,max_iter=30):\n",
    "    times = []\n",
    "\n",
    "    # z score for conf level\n",
    "    Z = norm.ppf((1 + conf_level) / 2)\n",
    "\n",
    "    ##run first empty once to get rid of startup overhead\n",
    "    run_csdm(engine, vox_per_chunk,False)\n",
    "    ##run a couple times to get standard deviation\n",
    "    print(\"running inital 6 to get standard deviation\")\n",
    "    for i in range(0,6):\n",
    "        times.append(run_csdm(engine, vox_per_chunk))\n",
    "\n",
    "    mean = statistics.mean(times)\n",
    "    std = statistics.stdev(times)\n",
    "\n",
    "    margin_err = conf_int * mean\n",
    "\n",
    "    n = math.ceil((Z * std / conf_int) ** 2)\n",
    "\n",
    "    print(\"mean: \", mean, \" std: \", std, \" margin_err: \", margin_err, \"conf_level: \", conf_level, \" n(needed to meet margin_err + conf_level): \", n)\n",
    "\n",
    "\n",
    "    # if we have enough samples, return,v\n",
    "    # else run more until we do\n",
    "    # if we run more than max_iter times, return\n",
    "    if (len(times) >= n):\n",
    "        print(\"enough samples taken\")\n",
    "        return\n",
    "    else:\n",
    "        while(len(times) < n):\n",
    "            times.append(run_csdm(engine, vox_per_chunk))\n",
    "\n",
    "            # update mean and std\n",
    "            mean = statistics.mean(times)\n",
    "            std = statistics.stdev(times)\n",
    "            margin_err = conf_int * mean\n",
    "            n = math.ceil((Z * std / conf_int) ** 2)\n",
    "\n",
    "            print(\"mean: \", mean, \" std: \", std, \" margin_err: \", margin_err, \"conf_level: \", conf_level, \" n(needed to meet margin_err + conf_level): \", n)\n",
    "\n",
    "            if(len(times) > max_iter):\n",
    "                print(\"max iterations reached\")\n",
    "                return\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting reconstruction model: : 3658350it [03:38, 16719.91it/s]                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine:  serial  vox_per_chunk:  1  time:  218.91965508460999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-01-30 10:49:58,296\tINFO worker.py:1724 -- Started a local Ray instance.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 2141 MiB, 6818 objects, write throughput 2044 MiB/s. Set RAY_verbose_spill_logs=0 to disable this message.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 5820 MiB, 18534 objects, write throughput 2704 MiB/s.\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 16885 MiB, 53774 objects, write throughput 3515 MiB/s.\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(raylet)\u001b[0m Spilled 32986 MiB, 105039 objects, write throughput 2872 MiB/s.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine:  ray  vox_per_chunk:  8  time:  127.1330931186676\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vox_per_chunk:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m engines:\n\u001b[0;32m---> 11\u001b[0m         \u001b[43mrun_csdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43mv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m         save_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunTimeData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m, in \u001b[0;36mrun_csdm\u001b[0;34m(engine, vox_per_chunk, save)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m runTimeData\n\u001b[1;32m     14\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 15\u001b[0m \u001b[43mcsdm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbrain_mask_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvox_per_chunk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvox_per_chunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     18\u001b[0m runTime \u001b[38;5;241m=\u001b[39m end\u001b[38;5;241m-\u001b[39mstart\n",
      "File \u001b[0;32m~/src/dipy/dipy/reconst/multi_voxel.py:78\u001b[0m, in \u001b[0;36mmulti_voxel_fit.<locals>.new_fit\u001b[0;34m(self, data, mask, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m [data_to_fit[ii:ii \u001b[38;5;241m+\u001b[39m vox_per_chunk]\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, data_to_fit\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], vox_per_chunk)]\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;66;03m##fits each chunk in parallel\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     fit_array[np\u001b[38;5;241m.\u001b[39mwhere(mask)] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((\n\u001b[0;32m---> 78\u001b[0m         \u001b[43mparamap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     79\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_parallel_fit_worker\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43msingle_voxel_with_self\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m             ))\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MultiVoxelFit(\u001b[38;5;28mself\u001b[39m, fit_array, mask)\n",
      "File \u001b[0;32m~/src/dipy/dipy/utils/parallel.py:89\u001b[0m, in \u001b[0;36mparamap\u001b[0;34m(func, in_list, out_shape, n_jobs, engine, backend, func_args, func_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     results \u001b[38;5;241m=\u001b[39m dask\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;241m*\u001b[39mdd, scheduler\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocesses\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     87\u001b[0m                            workers\u001b[38;5;241m=\u001b[39mn_jobs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m backend \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthreading\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 89\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mdask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m                           \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not a backend for dask\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m backend)\n",
      "File \u001b[0;32m~/src/parrallel_experiments/dipy/lib/python3.11/site-packages/dask/base.py:665\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    662\u001b[0m     postcomputes\u001b[38;5;241m.\u001b[39mappend(x\u001b[38;5;241m.\u001b[39m__dask_postcompute__())\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m shorten_traceback():\n\u001b[0;32m--> 665\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdsk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repack([f(r, \u001b[38;5;241m*\u001b[39ma) \u001b[38;5;28;01mfor\u001b[39;00m r, (f, a) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/queue.py:171\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_qsize():\n\u001b[0;32m--> 171\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m timeout \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimeout\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m must be a non-negative number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.11/3.11.6/Frameworks/Python.framework/Versions/3.11/lib/python3.11/threading.py:327\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 327\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    329\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "engines = [\"ray\",\"dask\"]\n",
    "\n",
    "vox_per_chunk = [2**i for i in range(3,17)]\n",
    "\n",
    "\n",
    "for i in range(30):\n",
    "    run_csdm(\"serial\",1)\n",
    "    save_data('runTimeData.csv')\n",
    "    for v in vox_per_chunk:\n",
    "        for e in engines:\n",
    "            run_csdm(e,v)\n",
    "            save_data('runTimeData.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csdm.fit(data, mask=brain_mask_data, engine=\"ray\", vox_per_chunk=(2**10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dipy",
   "language": "python",
   "name": "dipy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
