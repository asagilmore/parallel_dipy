{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(aws_access_key_id, aws_secret_access_key,num_chunks):\n",
    "    subject=100307\n",
    "    factor=1\n",
    "    model='csd'\n",
    "    engine='ray'\n",
    "    return_fit=False\n",
    "    \"\"\"\n",
    "    Runs the test with the given parameters.\n",
    "\n",
    "    Parameters:\n",
    "    aws_access_key_id (str): The AWS access key ID for accessing HCP data\n",
    "                             Keys can be obtained\n",
    "                             `here <https://db.humanconnectome.org/>`_.\n",
    "    aws_secret_access_key (str): The AWS secret access key for HCP\n",
    "    subject (int, optional): The subject ID to use in the test. Defaults to\n",
    "                             100307.\n",
    "    num_chunks (int, optional): The number of chunks to use in the test. Must\n",
    "                                be less than the number of voxels in the image.\n",
    "                                The number of computed chunks might be higher\n",
    "                                than the specified num_chunks due to intiger\n",
    "                                divison. Defaults to None.\n",
    "    factor (int, optional): The factor to use in the test. Defaults to 1.\n",
    "    model (str, optional): The model to use in the test. Defaults to 'csd'.\n",
    "    engine (str, optional): The engine to use in the test. Defaults to 'ray'.\n",
    "    return_fit (bool, optional): Whether to return the fit object. Defaults to\n",
    "                                 False.\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the results of the test.\n",
    "    \"\"\"\n",
    "\n",
    "    import os.path as op\n",
    "    from dipy.data.fetcher import fetch_hcp\n",
    "    from dipy.core.gradients import gradient_table\n",
    "    import nibabel as nib\n",
    "    import time\n",
    "    from dipy.align import resample\n",
    "    import numpy as np\n",
    "    import multiprocessing\n",
    "    import psutil\n",
    "    from nilearn import image\n",
    "    import threading\n",
    "    import dipy.reconst.fwdti as fwdti\n",
    "    import dipy.reconst.csdeconv as csd\n",
    "    from dipy.reconst.csdeconv import (mask_for_response_ssst,\n",
    "                                           response_from_mask_ssst)\n",
    "    import ray\n",
    "\n",
    "    def downsample(img, shape,factor=2):\n",
    "        shape = tuple(s // factor for s in shape)\n",
    "        img_resampled = image.resample_img(img, target_affine=img.affine,\n",
    "                                           target_shape=shape,\n",
    "                                           interpolation='continuous')\n",
    "        return img_resampled\n",
    "\n",
    "    class MemoryMonitor:\n",
    "        def __init__(self, interval):\n",
    "            self.interval = interval\n",
    "            self.memory_usage = []\n",
    "            self.stop_monitor = False\n",
    "\n",
    "        def monitor_memory(self):\n",
    "            while not self.stop_monitor:\n",
    "                mem_info = psutil.virtual_memory()\n",
    "                used_memory_GB = mem_info.used / (1024 ** 3)\n",
    "                self.memory_usage.append(used_memory_GB)\n",
    "                time.sleep(self.interval)\n",
    "\n",
    "        def get_memory_usage(self):\n",
    "            return self.memory_usage, (sum(self.memory_usage) / len(self.memory_usage))\n",
    "\n",
    "    dataset_path = fetch_hcp(subject, profile_name=False,\n",
    "                             aws_access_key_id=aws_access_key_id,\n",
    "                             aws_secret_access_key=aws_secret_access_key)[1]\n",
    "    subject_dir = op.join(dataset_path, \"derivatives\", \"hcp_pipeline\",\n",
    "                          f\"sub-{subject}\",)\n",
    "    subject_files = [op.join(subject_dir, \"dwi\", f\"sub-{subject}_dwi.{ext}\")\n",
    "                     for ext in [\"nii.gz\", \"bval\", \"bvec\"]]\n",
    "\n",
    "    dwi_img = nib.load(subject_files[0])\n",
    "\n",
    "    seg_img = nib.load(op.join(\n",
    "        subject_dir, \"anat\", f'sub-{subject}_aparc+aseg_seg.nii.gz'))\n",
    "\n",
    "    if(factor > 1):\n",
    "        shape = dwi_img.shape[:-1]\n",
    "        dwi_img = downsample(img=dwi_img, shape=shape,factor=factor)\n",
    "\n",
    "        shape = seg_img.shape\n",
    "        seg_img = downsample(img=seg_img, shape=shape,factor=factor)\n",
    "\n",
    "    seg_data = seg_img.get_fdata()\n",
    "    data = dwi_img.get_fdata()\n",
    "\n",
    "    brain_mask = seg_data > 0\n",
    "    dwi_volume = nib.Nifti1Image(data[..., 0], dwi_img.affine)\n",
    "    print(f'dwi_volume shape: {dwi_volume.shape}')\n",
    "    brain_mask_xform = resample(brain_mask, dwi_volume,\n",
    "                                moving_affine=seg_img.affine)\n",
    "    brain_mask_data = brain_mask_xform.get_fdata().astype(int)\n",
    "    gtab = gradient_table(subject_files[1], subject_files[2])\n",
    "\n",
    "    if model == 'csd':\n",
    "        response_mask = mask_for_response_ssst(gtab, data, roi_radii=10,\n",
    "                                               fa_thr=0.7)\n",
    "        response, _ = response_from_mask_ssst(gtab, data, response_mask)\n",
    "        model = csd.ConstrainedSphericalDeconvModel(gtab, response=response)\n",
    "    elif model == 'fwdti':\n",
    "        model = fwdti.FreeWaterTensorModel(gtab)\n",
    "\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    memory_size = psutil.virtual_memory().total\n",
    "\n",
    "    non_zero_count = np.count_nonzero(brain_mask_data)\n",
    "    vox_per_chunk = non_zero_count // num_chunks\n",
    "\n",
    "    monitor = MemoryMonitor(1)\n",
    "    monitor_thread = threading.Thread(target=monitor.monitor_memory)\n",
    "    monitor_thread.start()\n",
    "\n",
    "    start = time.time()\n",
    "    fit = model.fit(data, mask=brain_mask_data, engine=engine,\n",
    "                    vox_per_chunk=vox_per_chunk)\n",
    "    end = time.time()\n",
    "\n",
    "    monitor.stop_monitor = True\n",
    "    monitor_thread.join()\n",
    "\n",
    "    memory_usage, avg_memory_usage = monitor.get_memory_usage()\n",
    "    run_time = end - start\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    test_results = {'engine': engine, 'vox_per_chunk': vox_per_chunk,\n",
    "                            'num_chunks': num_chunks, 'time': run_time,\n",
    "                            'cpu_count': cpu_count,\n",
    "                            'memory_size': memory_size,\n",
    "                            'num_vox': non_zero_count,\n",
    "                            'avg_mem': avg_memory_usage,\n",
    "                            'mem_useage': memory_usage, 'model': model_name,\n",
    "                            'downsample_factor': factor, 'subject': subject,\n",
    "                            'data_shape': data.shape}\n",
    "\n",
    "    if return_fit:\n",
    "        return test_results, fit\n",
    "    else:\n",
    "        return test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudknot as ck\n",
    "import sys\n",
    "sys.path.append('./')\n",
    "\n",
    "image = ck.DockerImage(name=\"parallel-test-2\",func=run_test,\n",
    "                       base_image=\"python:3.11\",\n",
    "                       github_installs=\"https://github.com/asagilmore/dipy.git@parallel_test\",\n",
    "                       pin_pip_versions=True, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot = ck.Knot(name='parallel-experiment-3', docker_image=image, memory = 16000, job_def_vcpus=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from access_keys import aws_access_key, aws_secret_access_key\n",
    "args = [(aws_access_key,aws_secret_access_key,i) for i in range(10,100,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_futures = knot.map(args,starmap=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knot.view_jobs()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
